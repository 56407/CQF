% Default to the notebook output style
% Inherit from the specified cell style.
\documentclass[11pt]{article}
%    \usepackage[T1]{fontenc}
%    % Nicer default font than Computer Modern for most use cases
%    \usepackage{palatino}
    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}
    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
\usepackage[titletoc]{appendix}% http://ctan.org/pkg/appendices                                
                                
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}
    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{Learning Cointegration for Trading Strategies}
    \author{Tanya Sandoval}
    % Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother
    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    \begin{document}


    %%%%%%%%%%%%%%%%%%%%%%   TITLE
    \maketitle

%%%%%%%%%%%%%%%%%%%%%%   ABSTRACT
    \begin{abstract}
    \emph{This report introduces the topic of cointegration  and its application to trading strategies. 
    By modelling asset prices with an economic  link in common, it is sometimes possible to arrive at a stationary spread whose properties can be used to reduce exposure to systematic risk. The essential elements of cointegration such as stationary and mean-reversion are discussed, as well as some of the statistical tests available to detect this relationship. Simulated  and real data examples are provided, the latter focusing on a detected cointegration  between  Brent Crude and Gasoil futures. Lastly, examples of simple trading strategies applying cointegration are discussed, along with aspects that must be considered when trading under market real conditions. }
    \end{abstract}

%%%%%%%%%%%%%%%%%%%%%%   TABLE OF CONTENTS
    \newpage

    \tableofcontents
    \newpage


    \section{Introduction}\label{introduction}
    
    Historically  {\em cointegration} as a concept arose from statistical evidence that many US macroeconomic time series (like GDP, wages, employment, etc.) did not follow conventional econometric theory but rather were described by {\em unit root processes}, also known as  ``integrated of order 1'' I(1). Before the 1980s many economists used linear regressions on non-stationary time series, which Granger and Newbold showed to be a dangerous approach that could lead to  {\em spurious correlation}. For integrated I(1) processes, Granger and Newbold showed that de-trending does not work to eliminate the problem, and that the superior alternative is to check for cointegration, earning them the Nobel prize. 
    
    This report summarises some first learnings of this concept and first-steps at applying it to trading strategies. In particular, the focus is on studying two assets from the commodities space to see if similar properties can be detected as in equities.
    
\begin{itemize}
\tightlist
\item
  Section~\ref{datasets} provides details for the datasets used throughout the report
\item
  Section~\ref{stationarity-and-mean-reversion} introduces stationarity and mean-reversion in time series, which are key elements of cointegration
\item 
 Section~\ref{cointegration} goes into detail about cointegration and how to test for it,  as well as assessing its quality
\item
  Section~\ref{trading-strategies} is then dedicated to its application  to
  trading strategies and assessing their performance in terms of profit
  and loss (P\&L)
\item Appendix~\ref{multivariate-regression} summarises some of the mathematical methods
  involved, such as Multivariate Regression, Autoregressive models (AR(p)), Dickey-Fuller test, optimal lag order and stability conditions

\item Appendix~\ref{cointegration-between-italian-and-dutch-gas}  starts to examine  cointegration in other energy commodities, such as  Italian and Dutch gas

\end{itemize}

All the relevant scripts  to arrive at the results can be found in the project repository ``\emph{finalProject/TS}'' in the attached USB drive. In particular the ipython notebook \emph{Coint\_Brent\_Gasoil\_v2.ipynb}  demonstrates how to run the code, which is omitted in this report for brevity. The project repository is also available online on Github \url{https://github.com/tsando/CQF/tree/master/finalProject}.


    \section{Datasets}\label{datasets}
    
    
    \subsection{Simulated Data}\label{simulated-data}
    
    
Stochastic processes are used to  simulate asset prices. Monte
Carlo (MC) techniques are used when relevant and  random variables are drawn from the normal distribution $N(\mu=0, \sigma=1)$.


    \subsection{Real Data}\label{real-data}
    
    
    \begin{itemize}
\tightlist
\item
  For simplicity, only two financial series are used. As the focus is on  commodities, Brent crude and a
  by-product (Low Sulphur Gasoil) were selected since they were assumed
  to be good candidates for cointegration given their deep economic link
\item
  The
  Brent \cite{Brent}
  and
  Gasoil \cite{Gasoil}
  Futures prices traded in the Intercontinental Exchange (ICE) were
  taken from
  Quandl's  Steven Continuous Series \cite{Quandl'sStevenContinuousSeries} using the {\em Roll on Last Trading Day with No
  Price Adjustment} version and the {\em Settle} field as the closing price on the day
\item
  The two series were then joined to produce a single dataset consisting of
  daily settlement prices for Brent and Gasoil
\item
  The dataset spans 1.5 `trading years' (equivalent to $\sim 252$ days). The period selected was
  Jan-2014 to Dec-2014 for the in-sample testing and Jan-2015 to
  Jun-2015 for the out-of-sample testing. This was because several
  sources recommend to use one year of historic data to estimate the
  cointegration parameters and trade the estimates for a 6-month period,
  given that the parameters might change over time
\item
  Dates with missing values after joining the two series were removed from the dataset
\item
  Since Gasoil is traded in metric tons and Brent in barrels, the gasoil
  series was divided by 7.45, which is the
  ICE  conversion factor \cite{ICEconversionfactor}
\end{itemize}
The figure below shows the resulting dataset (spanning both in-sample
and out-of-sample periods), where the two series indeed seem to be
closely related, having very similar trends.
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_9_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    
    \section{Stationarity and Mean-Reversion}\label{stationarity-and-mean-reversion}
    
    Before cointegration is introduced, it is important to understand the
concept of {\em stationarity}. A time series is stationary when the parameters
of its generating process do not change over time. In particular, its
long-run mean and variance stay constant. This property is fundamental
when applying linear regression and forecasting models.
Often, processes with a drift or trend, like stock prices, are
non-stationary but can be transformed to become stationary. For example,
by differencing prices we get returns, which are in general stationary.
The figure below shows how a simulated random walk with drift
\(Y_t = \alpha + Y_{t-1} + \epsilon_t\) can be made stationary by
differencing it \( \Delta Y_t = Y_t - Y_{t-1} = \alpha + \epsilon_t\)
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_12_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    
    \subsection{Mean-Reversion}\label{mean-reversion}
    
    
A stationary series is {\em mean-reverting} if over time it drifts towards its
long-term mean (the historical equilibrium level). A popular model in
this category is the Ornstein--Uhlenbeck (OU) process:
\begin{equation}
dY_t = \theta(\mu - Y_t)dt + \sigma dW_t
\end{equation}
where \(\theta\) is the speed of reversion, \(\mu\) is the equilibrium
level, \(\sigma\) the variance and \(W_t\) is a Wiener Process (Brownian
Motion). In a discrete setting this states that the further away the
process is from the mean, the greater the `pull back' to it is. This is
in contrast to the random walk above, which has `no memory' of where
it has been at each particular instance of time.


The figure below shows three OU processes with the same mean \(\mu=10\)
but different mean-reversion speeds. Indeed it can be noted the one with the highest
$\theta$ reverts to the mean first. Their differences \(dY_t\) are plotted
as well  and these appear to become stationary significantly faster than
the process itself, almost insensitive to the speed \(\theta\).
Therefore, if we are able to transform a time series to be stationary
and mean-reverting, we can design trading strategies using these
properties which are more independent of market effects. In a later
section we shall see how the OU parameters can be used to design
exit/entry thresholds and also assess the {\em quality} of mean-reversion.
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_15_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    
    \subsection{Tests}\label{tests}
    
    
    We require a more robust method to confirm  whether a
series is stationary than just by eye. Several statistical tests exist, such as the \textbf{Augmented
Dickey-Fuller (ADF)} test, Phillips--Perron test, Hurst exponent, Kalman
filters, etc.
Here we only implement the ADF test and the mathematical details can be
found in Appendix~\ref{dickey-fuller-test-and-adf}.

    \subsubsection{Augmented Dickey-Fuller (ADF) }\label{adf-test-implementation}
    
The ADF test equation implemented was: \begin{equation}
\Delta Y_t = c_0 + \phi Y_{t-1} + \sum^p_k \phi_k \Delta Y_{t-k} + \epsilon_t 
\end{equation} where a time-trend term has not been included due to the nature of
financial time series \cite{refmissing}. The coefficients are
estimated using the familiar linear regression (see Appendix~\ref{multivariate-regression}) whereas
the optimal lag order \emph{p} is discussed below. The python script can
be found in \emph{analysis.py}. All the results were validated against
the popular python equivalents from the
{\em statsmodels} library \cite{statsmodelslibrary}.


    \paragraph{Optimal Lag Selection}\label{optimal-lag-selection}
    
    
Choice of lag order can be a difficult problem. Standard approaches use
an \emph{information criteria}, such as the Akaike Information Criterion
(AIC). However, different methods can lead to different results. Also,
keeping more lags can lead to \emph{model overfitting}. In practice, the
choice of optimal lag is also evident from the
Partial Autocorrelation Function \cite{PartialAutocorrelationFunction} (PACF) since the significant lags would show
above confidence limits.
Typically for the ADF test it is enough to take \emph{p=1}, however in
the interest of exploring this aspect, here we look at the results using
AIC and PACF.
\begin{itemize}
\tightlist
\item
  \textbf{AIC:} Iterating over different lag orders, the one yielding
  the lowest AIC value is taken as the optimal lag. For the simulated
  random-walk-with-drift and OU processes above the results are
  summarised in the table below.
\end{itemize}
    \begin{longtable}[]{@{}lll@{}}
\toprule
Process & OU $\theta$ & AIC optimal lag\tabularnewline
\midrule
\endhead
$Y_{t}$ & 0 & 22\tabularnewline
$Y_{t1}$ & 0.003 & 1\tabularnewline
$Y_{t2}$ & 0.010 & 10\tabularnewline
$Y_{t3}$ & 0.100 & 2\tabularnewline
\bottomrule
\end{longtable}
    \begin{itemize}
\tightlist
\item
  \textbf{PACF:} Given that the optimal lag order from AIC comes out
  quite high in some cases, we instead use the
  empirical results from the PACF plot below, where it can be seen only the first lag is well above the 95\% confidence band (the
  first `spike' represents \emph{p=0}). Given this, we therefore assume
  it is `safe' to take \emph{p=1} to carry out the ADF test.
\end{itemize}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_25_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \paragraph{ADF}\label{adf}
    
Using as optimal lag \emph{p=1}, we run the ADF test and compare the
corresponding t-statistic to the critical values (taken from
statsmodels, based on
MacKinnon(2010) \cite{MacKinnon(2010)}). The results are summarised in the table below, where we confirm what we expected: the null hypothesis of non-stationary is rejected for all, except for $Y_t$, which by definition is non-stationary given its drift
    \begin{longtable}[]{@{}lllllll@{}}
\toprule
Process & $\theta$ & ADF t-stat & 5\% Crit. Val. & p-value & Stationary & Stable\tabularnewline
\midrule
\endhead
$Y_{t}$ & 0 & 0.1093 & -2.8644 & 0.9667 & No & Yes\tabularnewline
$Y_{t1}$ & 0.003 & -6.0020 & -2.8644 & 1.65E-07 & Yes & Yes\tabularnewline
$Y_{t2}$ & 0.01 & -8.7597 & -2.8644 & 2.69E-14 & Yes & Yes\tabularnewline
$Y_{t3}$ & 0.1 & -10.1133 & -2.8644 & 9.87E-18 & Yes & Yes\tabularnewline
$dY_{t}$ & 0 & -31.8476 & -2.8644 & 0.0 & Yes & No\tabularnewline
$dY_{t1}$ & 0.003 & -29.4587 & -2.8644 & 0.0 & Yes & Yes\tabularnewline
$dY_{t2}$ & 0.01 & -29.3639 & -2.8644 & 0.0 & Yes & Yes\tabularnewline
$dY_{t3}$ & 0.1 & -31.3506 & -2.8644 & 0.0 & Yes & No\tabularnewline
\bottomrule
\end{longtable}
    \paragraph{Stability Check}\label{stability-check}
To ensure further the reliability of results, a stability check can be
done on the estimated coefficients by looking at their eigenvalues
within the unit circle (see Appendix~\ref{stability-condition}). The results of the
self-implementation are displayed in the table above. All cases were
found stable, except $dY_{t}$ and $dY_{t3}$. This demonstrates that
stationarity does not imply stability. The unstable nature of $dY_{t}$ may
be due to the drift term added \(dY_t = \alpha + \epsilon_t\). A close
inspection of the problematic root of $dY_{t3}$ shows it is just right on
the boundary of the unit circle.

    \subsection{Other useful plots}\label{other-useful-plots}
    
In addition to the PACF, when assessing stationarity the below plot types are useful (see
references \cite{pandasref1} and \cite{pandasref2}):
\begin{itemize}
\tightlist
\item
  \textbf{Autocorrelation plot:} This shows the autocorrelation function
  (ACF) at varying time lags. For perfectly stationary series or independent and identically distributed (iid)
  random variables, the autocorrelations should be near zero for all
  time-lag separations. The horizontal lines displayed in the plot
  correspond to 95\% and 99\% confidence levels. Indeed in the example below none of
  the $dY_t$  processes show significant autocorrelation. Also, the
  OU process with the highest mean-reversion speed $Y_{t3}$ rapidly loses
  autocorrelation to its lags, and looking stationary.
\end{itemize}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_32_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{itemize}
\tightlist
\item
  \textbf{Lag plot:} this is a scatter plot between the series \(Y_t\)
  and one of its lags \(Y_{t-p}\). Like in the autocorrelation plot, a
  stationary series would not exhibit any relationship. Below an example
  is shown for two of the OU processes and their differences. This
  confirms the results from the autocorrelation plot - the fastest
  mean-reverting $Y_{t3}$ has little relation to the 10th lag, unlike $Y_{t1}$,
  which has a lower speed. As expected, the stationary differences are
  insensitive to the lags, even for \(Y_{t-1}\).
\end{itemize}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_34_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \section{Cointegration}\label{cointegration}
    Having covered the key concepts to understand cointegration, we proceed
to explain this topic.
Two or more time series $\mathbf{Y}_{\emph{t}} = (y_{1t}, \dots,
y_{nt})^{\prime} $ are said to be cointegrated if a linear
combination exists which makes the collection `integrated of order
zero' I(0) i.e stationary:
\begin{equation}
\mathbf{\beta^\prime Y}_t = \beta_1 y_{1t} + \dots + \beta_n y_{nt} \sim I(0)
\end{equation}
This is known as the \emph{long-run (static) equilibrium} model and is
expressed in normalised form as:
\begin{equation}
y_{1t} = \beta_2 y_{2t} + \dots + \beta_n y_{nt} + e_t
\end{equation}
where the residual \(e_t \sim I(0)\) is referred to as the
\emph{cointegrating residual/spread} and
\(\mathbf{\beta} = (1, -\beta_2, \dots, -\beta_n)^\prime\) is the
\emph{cointegrating vector}.
Due to time constraints, this report covers only a pair of
cointegrated time series, however the concept can be extended to more
series and higher orders of integration.
In real data, cointegration usually exists when there is a deep
economic link between the assets and hence these cannot drift too far
apart because economic forces will act to restore the long-run
equilibrium. The figure below shows a simulated pair of cointegrated
assets \(y_{1t}\) and \(y_{2t}\), where \(y_{2t}\) is defined as an I(1)
process. If \(y_{1t}\) is supposed to have a strong link to \(y_{2t}\),
the price of \(y_{1t}\) should vary similarly. This is simulated by
shifting up \(y_{2t}\) and adding some noise (residual) drawn from a
normal distribution, so \(y_{1t}\) is defined as the dependent variable
and \(y_{2t}\) as the independent variable:
\begin{equation}
y_{1t} = y_{2t} + 5.0 + \epsilon_t
\end{equation}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_37_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    \subsection{Tests}\label{tests}
Again, we need a robust statistical test to confirm cointegration. The
three main approaches are:
\begin{itemize}
\item
  \textbf{Engle--Granger two-step method:} This can only be used to test
  a \emph{single} cointegrating relationship. The steps are:
  \begin{enumerate}
  \def\labelenumi{(\roman{enumi})}
  \item
    Estimate the cointegrating residual $\hat{e}_t =
    \mathbf{\hat{\beta}^\prime} \mathbf{Y}_t$, e.g.~using linear
    regression
  \item
    Test \(\hat{e}_t\) for stationarity, e.g.~using the ADF test, where
    the hypotheses to be tested are:
  \end{enumerate}
  \begin{equation}
  H_0 \space :  \space \hat{e}_t = \mathbf{\hat{\beta}^\prime} \mathbf{Y}_t \sim I(1) \qquad \mathrm{(no \text{ } cointegration)}
  \end{equation}
  \begin{equation}
  H_1 \space :  \space \hat{e}_t = \mathbf{\hat{\beta}^\prime} \mathbf{Y}_t \sim I(0) \qquad \mathrm{(cointegration)}
  \end{equation}
\end{itemize}
We refer to this use case of the ADF as `CADF'
\begin{itemize}
\item
  \textbf{Johansen test:} Based on maximum likelihood techniques, this
  allows for more than one cointegrating relationship, but it is subject
  to asymptotic conditions when the sample size is too small
\item
  \textbf{Phillips--Ouliaris test:} Uses a modified version of the
  Dickey-Fuller distribution to test the cointegrating spread for
  stationarity. This is a better choice when dealing with small samples
\end{itemize}
In this project only the Engle-Granger two-step method is considered.

    \subsubsection{Engle-Granger Two-Step}\label{engle-granger-test-implementation}

    For our simulated cointegrated series, the cointegration relationship is
then represented by the regression:
\begin{equation}
y_{1t} = c + \beta_2^\prime y_{2t} + e_t
\end{equation}
whose parameters are estimated using OLS. Hence, the estimated
cointegrating spread is:
\begin{equation}
\hat{e}_t = y_{1t} - \hat{c} - \hat{\beta}_2^\prime y_{2t}
\end{equation}
We then test \(\hat{e}_t\) for stationarity using ADF. Since the mean of
\(\hat{e}_t\) is zero, the ADF can be implemented without a constant or
trend (See reference \cite{refp445}). Also note that the critical values used are taken as in
statsmodels from MacKinnon (2010) \cite{(seeref)}, whereas other sources suggest to use the Phillips-Ouliaris
tabulation in this case.
The figure below shows the OLS fit:
\begin{equation}
\hat{y}_{1t} = 5.0330 + 1.0052 y_{2t} + \hat{e}_t
\end{equation}
which is in good agreement with its true value. Plotted is also the
estimated \(\hat{e}_t\) and its PACF. The PACF shows the spread is
\emph{memoryless}, i.e.~no lag order appears significant so it is a
random (Markov) process.
Although \(\hat{e}_t\) appeared memoryless, conservatively the CADF
t-statistic was computed using one lag. This was \(-12.051\), which was
below the \(1\%\) critical value (\(-2.5748\)), confirming the
stationarity of the spread, which by design we expected.
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_42_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_43_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \subsection{Error Correction Model
(ECM)}\label{error-correction-model-ecm}
Cointegration implies the existence of an Error Correction Model (ECM),
which provides an adjustment to the long-run equilibrium from the
short-run dynamics. This is particularly useful when modelling
non-stationary series, like market prices, which can lead to spurious
regression results.
Suppose the cointegrated pair is represented by $\mathbf{Y}_t = (y_t,
x_t)^{\prime} $. One can arrive at the ECM result as follows:
\begin{itemize}
\tightlist
\item
  Consider a dynamic regression model to allow for a wide variety of
  dynamic patterns in the data. This is done by including lags for both
  \(x_t\) and \(y_t\):
\end{itemize}
\begin{equation}
y_t = \alpha y_{t-1} +  \beta_0 + \beta_1 x_t + \beta_2 x_{t-1} + \epsilon_t
\end{equation}
\begin{itemize}
\item
  By knowing the above equation should be consistent with the long-run
  equilibrium model \(y_t = b_0 + b_1 x_t + e_t\), it can be re-written
  as:
  \begin{equation}
  \Delta y_{t} = \beta_1 \Delta x_t - (1- \alpha) e_{t-1} + \epsilon_t
  \end{equation}
  where \(e_{t-1}\) is the lagged cointegrating spread from the
  equilibrium model. The parameter \(-(1-\alpha)\) is interpreted as the
  speed of correction towards the equilibrium level (more details on this in the next
  section)
\item
  Since all the variables in the ECM are I(0), OLS can be used to
  estimate the parameters
\end{itemize}
    \subsection{Quality of mean-reversion}\label{quality-of-mean-reversion}
If the cointegrating spread \(e_t\) is stationary, we could use the OU
process to model it:
\begin{equation}
d e_t = \theta (\mu_e - e_t) dt + \sigma dW
\end{equation}
In discrete time this can be written as:
\begin{equation} 
\Delta e_t = \alpha \mu_e - \alpha e_{t-1} + \epsilon_{t, \tau}
\end{equation}
where \(\alpha= 1 - e^{-\theta \tau}\) and \(\tau\) is a small period of
time. This is in fact the implied ECM representation for \(e_t\). As
discussed in the Appendix, the above equation can be written in its
AR(1) representation as:
\begin{equation}
e_t = \alpha \mu_e + (1-\alpha)e_{t-1} + \epsilon_{t, \tau}
\end{equation}
Re-writing in terms of \(C = \alpha \mu_e\) and \(B=1-\alpha\) we see it
is a simple regression which can be determined with OLS:
\begin{equation} 
e_t = C + B e_{t-1} + \epsilon_{t, \tau} \\
\end{equation}
In particular, to assess the quality of the spread for trading
strategies the parameters of interest are:
\begin{equation}
\theta = -\frac{\ln B}{\tau} \qquad \mu_e = \frac{C}{1-B} 
\quad \sigma_{OU} = \sqrt{\frac{2 \theta}{1-e^{-2\theta \tau}} Var[\epsilon_{t, \tau}]}
\end{equation}
\begin{itemize}
\item
  \(\mu_e\) is the long-run equilibrium level of the OU process
\item
  The mean-reversion speed \(\theta\) can be translated into a half-life
  $\tilde{\tau} \propto \ln 2 / \theta $, which is the time between
  equilibrium situations, i.e.~when \(e_t = \mu_e\). Hence, a high
  \(\theta\) (small \(\tilde{\tau}\)) is desirable to trigger trading
  signals
\item
  The standard deviation defined as
  \(\sigma_{eq} = \sigma_{OU} / \sqrt{2 \theta}\) can be used to plot
  trading bounds for entry/exit signals as \(\mu_e \pm \sigma_{eq}\)
\item
  Here we use \(\tau = 1/252\) because our data has a daily frequency and
  the in-sample period covers one trading year, i.e. \(\sim 252\) trading
  days
\end{itemize}
    \subsection{Real Data Example}\label{real-data-example}
    We now proceed to apply the concepts discussed above to real data:
\begin{itemize}
\item
  We use the in-sample dataset for Brent crude and Gasoil (Jan-2014 to
  Dec-2014). The out-of-sample dataset is later used to backtest trading
  strategies
\item
  For now we assume Brent is the independent variable \(x_t\) and Gasoil
  the dependent variable \(y_t\), and later check if this is accurate
  via \emph{Granger's causality test}
\end{itemize}
    \subsubsection{Stationarity}\label{stationarity}
First we check the individual price series for unit root I(1). We apply the ADF test using one lag only as recommended by
their PACF plot below. A drift term (constant) is also included in the
test. The results are summarised in the table below and support the assumption that Brent
and Gasoil are I(1) processes whilst their differences are I(0).
    \begin{longtable}[]{@{}llllll@{}}
\toprule
Series & ADF t-stat & 5\% Crit. Val. & p-value & Stationary &
Stable\tabularnewline
\midrule
\endhead
\(x_t\) (Brent) & 3.5768 & -2.8728 & 1.00 & No & No\tabularnewline
\(y_t\) (Gasoil) & 3.9374 & -2.8728 & 1.00 & No & No\tabularnewline
\(\Delta x_t\) & -18.1437 & -2.8728 & 2.49E-30 & Yes &
Yes\tabularnewline
\(\Delta y_t\) & -18.9247 & -2.8728 & 0.00 & Yes & Yes\tabularnewline
\bottomrule
\end{longtable}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_52_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    \subsubsection{Cointegration}\label{cointegration2}
We then proceed to test whether the pair is cointegrated, using the
Engle-Granger two-step procedure previously described.
\begin{itemize}
\item
  The long-run equilibrium model is shown in the figure below. The OLS
  estimate was:
  \begin{equation}
   \hat{y}_x = 16.3229 + 0.9699 x_t + \hat{e}_t
  \end{equation}
  with goodness-of-fit \(R^2=0.986\).
\end{itemize}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_57_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{itemize}
\item
  The estimated cointegrating spread \(\hat{e}_t\) series, histogram and
  PACF are shown below. First the mean of \(\hat{e}_t\) is zero for all
  practical purposes (\(\sim10^{-13}\)).
\item
  We also want \(\hat{e}_t\) to be normally distributed. The histogram
  below shows the spread distribution fitted to a normal distribution
  with \(\mu \sim 0\) and \(\sigma=1.66\). Two tests (\emph{Lilliefors}
  and \emph{Anderson-Darling}, see
  references \cite{ref1} and \cite{ref2})
  were carried out to assess the goodness of fit and neither rejected the
  null hypothesis that \(\hat{e}_t\) is normally distributed
\item
  Next, we note that the PACF plot shows a peculiarity of this spread -
  it seems to have an AR(3) `memory', given the first three lags appear
  significant. Memory in the cointegrating spread is actually \emph{not
  desirable} because it will interfere with both mean-reversion and
  diffusion components of the OU process. One hypothesis is that this
  has an `economic explanation' in terms of commodity pricing,
  cyclicality or storage (e.g.~asset cannot dramatically drop because
  there are storage/delivery `carry' costs borne by the sellers)
\item
  Next, \(\hat{e}_t\) was tested for stationarity using the CADF test.
  Since \(\hat{\mu}_e \sim 0\) the test was implemented without a
  constant or trend. Also, as suggested by the PACF plot, the test was
  ran with 3 lags. The results below show stationary could only be
  confirmed at the 95\% C.L.
\end{itemize}
    \begin{longtable}[]{@{}lllllll@{}}
\toprule
\begin{minipage}[b]{0.07\columnwidth}\raggedright\strut
Series\strut
\end{minipage} & \begin{minipage}[b]{0.10\columnwidth}\raggedright\strut
ADF t-stat\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedright\strut
5\% Crit. Val.\strut
\end{minipage} & \begin{minipage}[b]{0.13\columnwidth}\raggedright\strut
1\% Crit. Val.\strut
\end{minipage} & \begin{minipage}[b]{0.08\columnwidth}\raggedright\strut
p-value\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright\strut
Stationary (95\% C.L.)\strut
\end{minipage} & \begin{minipage}[b]{0.07\columnwidth}\raggedright\strut
Stable\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
\(\hat{e}_t\)\strut
\end{minipage} & \begin{minipage}[t]{0.10\columnwidth}\raggedright\strut
-2.2335\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
-1.9421\strut
\end{minipage} & \begin{minipage}[t]{0.13\columnwidth}\raggedright\strut
-2.5746\strut
\end{minipage} & \begin{minipage}[t]{0.08\columnwidth}\raggedright\strut
0.0245\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright\strut
Yes\strut
\end{minipage} & \begin{minipage}[t]{0.07\columnwidth}\raggedright\strut
Yes\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_60_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    \subsubsection{ECM}\label{ecm}
The corresponding ECM adjustment was determined to be:
\begin{equation}
\begin{array}{ccc}
\Delta y_{t}  = & -0.0861 + & 0.6455 \Delta x_t -0.1623 e_{t-1} + \epsilon_t \qquad \mathrm{(with~constant,~R^2=0.407)} \\
\Delta y_{t}  = & & 0.6596 \Delta x_t -0.1633 e_{t-1} + \epsilon_t
\qquad \mathrm{(no~constant, ~R^2=0.422)}
\end{array}
\end{equation}
This represents the second-order adjustment to the price, which as we
see is quite significant.
    \subsubsection{Quality of
mean-reversion}\label{quality-of-mean-reversion}
    Given the peculiar PACF of \(\hat{e}_t\), which suggests it is an AR(3)
process, fitting to the OU process, which is an AR(1) process, might not
be suitable. We proceed nevertheless to compare the results between both
fits. For AR(3) we take the OU parameters as for AR(1): the constant
term equals \(\alpha \mu_e\) and the coefficient of the \(e_{t-1}\) term
is \(\alpha = 1 - e^{-\theta \tau}\). The table below shows the results
obtained, where we see the trading bounds are:
\begin{equation}
 \mu_e \pm \sigma_{eq} =
  \begin{cases}
    -0.1255 \pm  1.6541 & \quad \text{AR(1)} \\
    -0.0529 \pm  0.8820  & \quad \text{AR(3)}\\
  \end{cases}
\end{equation}
The AR(3) fit yields a lower standard error in \(\theta\) than AR(1),
although not by much and yet the \(\theta\) values are quite different.
This perhaps indicates the OU fit is not suitable regardless of the lag
order.
    \begin{longtable}[]{@{}ccccccc@{}}
\toprule
Process & \(\theta\) & \(\tilde{\tau}\) & \(\mu_e\) & \(\sigma_{OU}\) &
\(\sigma_{eq}\) & \(s.e._\theta\)\tabularnewline
\midrule
\endhead
AR(1) & 49.1253 & 0.0141 & -0.1255 & 16.3957 & 1.6541 &
7.3850\tabularnewline
AR(3) & 258.9327 & 0.0027 & -0.0529 & 20.0723 & 0.8820 &
5.6638\tabularnewline
\bottomrule
\end{longtable}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_73_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    
    \subsection{Granger Causality}\label{granger-causality}
    
    
When implementing the cointegrating regression to estimate \(e_t\), an
assumption must be made about which the dependent variable is. It is
actually important to determine the best choice for the independent
variable and the dependent variable, as this can largely influence the
parameters estimates and test results.
\begin{itemize}
\tightlist
\item
  Optimal lag determined from the PACF plot of Brent and Gasoil, which
  shows only maxlag=1 is necessary
\item
  Only do this test in the in-sample, assuming the relationship holds in
  the out-of-sample
\end{itemize}


    \subsection{Issues with Cointegration}\label{issues-with-cointegration}
    
\begin{itemize}
\tightlist
\item
  Johansen procedure - MLE for multivariate cointegration on asset price
  data (levels, not returns)
\end{itemize}


    \section{Trading Strategies}\label{trading-strategies}
    
    Below we describe a few strategies which exploit the cointegration
properties. It should be noted that it is actually very difficult to
find a directly tradable asset that possesses mean-reverting behaviour -
in practice this is instead achieved by creating a \emph{stationary
portfolio} of separately interrelated assets. For the demonstrations
below however, we remain with a the 2-asset portfolio from before: Brent
and Gasoil.

    \subsection{Regime changes}\label{regime-changes}
    
Tests for cointegration assume that the cointegrating vector is constant
during the period of study. In reality, it is possible that the long-run
relationship between the underlying variables change (shifts in the
cointegrating vector can occur), e.g from changes in their
\emph{fundamental factors}. This is especially likely to be the case if
the sample period is long. Hence, it should not be assumed that because
the assets have passed a cointegration test historically, they will
continue to remain cointegrated. Practitioners' advice is to estimate
using one year of historic data and trade the estimates for a 6-month
period.
This is then a good introduction to the next topic, which is
backtesting.

    \subsection{Backtesting}\label{backtesting}
    
Backtesting is the process of testing a trading strategy or model on
historical data to gauge its effectiveness. When we backtest a model,
the results achieved are highly dependent on the tested period and this
can cause the strategy to fail in the future, as regime changes or
\emph{model overfitting} could have taken place.
To alleviate this, the test data is usually split into \emph{in-sample}
and \emph{out-of-sample} (a `rule-of-thumb' is to use an 80-20 split).
The model/strategy is then fitted/tested to the in-sample data
\emph{only} and then tested on the out-of-sample data, which was
`unseen'. This process provides a better way to assess the true
performance.

    \subsection{Naive Beta-Hedging Strategy}\label{naive-beta-hedging-strategy}
    
    ** Factor models** are a way of explaining the returns of one asset via
a linear combination of the returns of other assets. This can be
expressed as a simple linear regression:
\begin{equation}\Delta Y = \alpha + \beta_1 \Delta X_1 + \beta_2 \Delta X_2 + \dots + \beta_n \Delta X_n + \epsilon_t \end{equation}
We can interpret the betas as the \emph{exposure} of asset \(Y\) to the
other assets and \(\alpha\) as the market-neutral excess return:
\begin{equation} \alpha = \mathrm{E}[\Delta Y - \beta_1 \Delta X_1 + \beta_2 \Delta X_2 + \dots + \beta_n \Delta X_n] \end{equation}
since \(\mathrm{E}[\epsilon_t] =0\).
For the case of Brent and Gasoil only this would be:
\begin{equation}\Delta Y_{gasoil} = \alpha + \beta \Delta X_{brent} + \epsilon_t \end{equation}
We could take a short position in Brent equal to
\(\beta \Delta X_{brent}\) to try to eliminate the \emph{risk} in our
Gasoil position. Hence, we would expect our returns to be on average:
\begin{equation}\mathrm{E} [\Delta Y_{gasoil} - \beta X_{brent}] = \alpha \end{equation}
For the \textbf{in-sample} data, running the above regression would
actually yield negative returns, although less severe than just going
long on Gasoil:
\begin{equation}
\begin{array}{rl}
\mathrm{E} [ \Delta Y_{gasoil}^{is}  -0.6282 \Delta X_{brent}^{is}] =  & \alpha   =  -0.0915 \qquad (R^2 = 0.354) \\
\mathrm{E} [ \Delta Y_{gasoil}^{is}]  =  & \alpha   =   
-0.2148  \qquad  \text{(no fit)}
\end{array}
\end{equation}
One problem could be that the estimated beta is not constant as we walk
forward in time. As such, the short position we took out in Brent is not
perfectly hedging our portfolio. Another is that additional factors or
assets should be included into the model.
Surprisingly, our performance in the \textbf{out-of-sample} seems to be
better but that was just mere `luck' from positive fluctuations in the
data, and still worse off than just going long on Gasoil over this period:
\begin{equation}
\begin{array}{rl}
\mathrm{E} [ \Delta Y_{gasoil}^{os}  -0.6282 \Delta X_{brent}^{os}] =  & \mathrm{E}[ \alpha_t]    = 0.02231 \qquad \text{(in-sample fit)}  \\
\mathrm{E} [ \Delta Y_{gasoil}^{os}] =   & \mathrm{E}[ \alpha_t]   =  0.05806  \qquad  \text{(no fit)}
\end{array}
\end{equation}
    \subsection{Pairs Trading Strategy}\label{pairs-trading-strategy}
    Moving to a more sophisticated approach, we now apply the concepts of
cointegration learned. Although it is commonly referred as `pairs
trading', the concept applies to any stationary portfolio of assets.
For a pair of assets, if they are cointegrated, e.g.~because they belong
to the same `sector', they are likely to be exposed to similar market
factors. Occasionally their relative prices will diverge due to certain
events, but eventually will revert to the long-run equilibrium. Hence,
positions are taken relative to where the \textbf{cointegrating spread
\(\hat{e}_t\)} is with respect to the equilibrium level. The objective
is to hedge the position from price level dynamics (market risk). The
P\&L generated will then be driven by the quality of mean-reversion.
For Gasoil and Brent, we estimated the \textbf{in-sample} cointegrating
relationship as:
\begin{equation}
Y_{gasoil}^{is} = 0.9699 X_{brent}^{is}  + 16.3229 + \hat{e}_t^{is}
\end{equation}
Under the assumption that the relationship holds, we use the same beta
and constant to estimate the \textbf{out-of-sample} spread:
\begin{equation}
\hat{e}_t^{os} = Y_{gasoil}^{os} - 0.9699 X_{brent}^{os}  - 16.3229 
\end{equation}
This is then what we use to \textbf{backtest} the strategy.
    \subsubsection{Bounds from OU process}\label{bounds-from-ou-process}
As we saw, we could define thresholds for trading the spread by fitting
to the OU process and using the fitted mean \(\mu_e\) and relative
standard deviation \(\sigma_{eq}\). Assuming normality of the spread
\(e_{t, \tau \to \infty} \sim N (\mu_e, \sigma^2_{eq})\), a z-score
could be used to `normalise' the spread:
\begin{equation}
z = \frac{\hat{e}_t - \mu_e}{\sigma_{eq}}
\end{equation}
Given this, a simple strategy would then be:
\begin{itemize}
\item
  Go ``Long'' the spread when the z-score is below -1.0: this means
  longing \(Y_{gasoil}\) and shorting \(X_{brent}\)
\item
  Go ``Short'' the spread when the z-score is above 1.0: this means
  shorting \(Y_{gasoil}\) and longing \(X_{brent}\)
\item
  Exit positions when the z-score is within \([-0.1, 0.1]\), i.e.~near
  zero
\end{itemize}
The figure below shows the out-of-sample z-score spread with the AR(3)
OU entry/exit bounds (Eqn. 29).
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_97_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    The corresponding P\&L using the AR(3) bounds is shown below for both
the in-sample and out-of-sample periods. A positive performance is
observed throughout, although with prolonged drawdown periods. In
addition, note this does not include \textbf{transaction costs} which
could certainly add up to an overall loss. The P\&L using the AR(1)
bounds is also shown, where it is clear it is worse off than AR(3). Both
however are better off than having simply gone long in Gasoil or Brent -
this is taken as the market `benchmark'.
Given the positive results, it would be interesting to see the P\&L from
extending to more than 2 assets cointegrated with oil, with a strategy
that trades multiple cointegrated spreads at the same time, or a single
spread composed of more factors. In this case, the \textbf{Johansen
test} would need to be implemented instead of CADF.
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_101_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \subsubsection{Bounds from Optimisation}\label{bounds-from-optimisation}
Although earlier the Gasoil-Brent spread passed the Anderson-Darling
test for normality, we also saw it had memory AR(3). Hence, it is
perhaps not optimal to use an OU fit to define the bounds, as this is
only suited for an AR(1) spread. The P\&L plot also suggests the
performance is sensitive to where bounds are. This motivates the search
for the \emph{optimal bound}, which maximises the P\&L or any other
P\&L-related metric, e.g.~number of trades. Optimisation techniques can
be used to test different combinations of \(\mu_e \pm \sigma_{eq}\) in
the in-sample data, although again, this can suffer from model
overfitting and cause a worse performance in the out-of-sample. Due to
time constraints this was not evaluated in this report.
    \subsubsection{Dynamic Bounds}\label{dynamic-bounds}
Another alternative is to use rolling values for
\(\mu_e \pm \sigma_{eq}\) instead of static. Again, an optimisation
could be done to find the optimal period for the rolling window,
which could be different for \(\mu_e\) and \(\sigma_{eq}\) given their
different sensitivity to time-scales.
    
    
    %%%%%%%%%%%%%%%%%%%%%%   BIBLIOGRAPHY
\newpage

\addcontentsline{toc}{section}{References}
    \begin{thebibliography}{1}
\bibitem{ICEconversionfactor} \href{https://www.theice.com/publicdocs/futures/ICE\_Gas\_Oil\_Crack.pdf}{https://www.theice.com/publicdocs/futures/ICE\_Gas\_Oil\_Crack.pdf}
\bibitem{PartialAutocorrelationFunction} \href{http://nl.mathworks.com/help/econ/autocorrelation-and-partial-autocorrelation.html}{http://nl.mathworks.com/help/econ/autocorrelation-and-partial-autocorrelation.html}
\bibitem{pandasref1} \href{http://pandas.pydata.org/pandas-docs/stable/visualization.html\#autocorrelation-plot}{http://pandas.pydata.org/pandas-docs/stable/visualization.html\#autocorrelation-plot}
\bibitem{pandasref2} \href{https://github.com/pydata/pandas/blob/master/pandas/tools/plotting.py}{https://github.com/pydata/pandas/blob/master/pandas/tools/plotting.py}
\bibitem{Gasoil} \href{https://www.theice.com/products/34361119/Low-Sulphur-Gasoil-Futures}{https://www.theice.com/products/34361119/Low-Sulphur-Gasoil-Futures}
\bibitem{statstools} \href{http://statsmodels.sourceforge.net/stable/vector\_ar.html\#lag-order-selection}{http://statsmodels.sourceforge.net/stable/vector\_ar.html\#lag-order-selection}
\bibitem{wikiAR} \href{https://en.wikipedia.org/wiki/Autoregressive\_model}{https://en.wikipedia.org/wiki/Autoregressive\_model}
\bibitem{ref2} \href{http://statsmodels.sourceforge.net/devel/generated/statsmodels.stats.diagnostic.kstest\_normal.html}{http://statsmodels.sourceforge.net/devel/generated/statsmodels.stats.diagnostic.kstest\_normal.html}
\bibitem{wikiVAR} \href{https://en.wikipedia.org/wiki/Vector\_autoregression}{https://en.wikipedia.org/wiki/Vector\_autoregression}
\bibitem{wikiADF} \href{https://en.wikipedia.org/wiki/Augmented\_Dickey\%E2\%80\%93Fuller\_test}{https://en.wikipedia.org/wiki/Augmented\_Dickey\%E2\%80\%93Fuller\_test}
\bibitem{refmissing} \href{}{}
\bibitem{Brent} \href{https://www.theice.com/products/219/Brent-Crude-Futures}{https://www.theice.com/products/219/Brent-Crude-Futures}
\bibitem{(seeref)} \href{http://statsmodels.sourceforge.net/stable/generated/statsmodels.tsa.stattools.adfuller.html\#statsmodels.tsa.stattools.adfuller}{http://statsmodels.sourceforge.net/stable/generated/statsmodels.tsa.stattools.adfuller.html\#statsmodels.tsa.stattools.adfuller}
\bibitem{wikiDF} \href{https://en.wikipedia.org/wiki/Dickey\%E2\%80\%93Fuller\_test}{https://en.wikipedia.org/wiki/Dickey\%E2\%80\%93Fuller\_test}
\bibitem{MacKinnon(2010)} \href{http://statsmodels.sourceforge.net/stable/generated/statsmodels.tsa.stattools.adfuller.html\#statsmodels.tsa.stattools.adfuller}{http://statsmodels.sourceforge.net/stable/generated/statsmodels.tsa.stattools.adfuller.html\#statsmodels.tsa.stattools.adfuller}
\bibitem{refp445} \href{file:///C:/Users/Tanya.Sandoval/Downloads/Cointegration\%20-\%20Book\%20Chapter\%20-\%20UWashington\%20E\%20Zivot.pdf}{file:///C:/Users/Tanya.Sandoval/Downloads/Cointegration\%20-\%20Book\%20Chapter\%20-\%20UWashington\%20E\%20Zivot.pdf}
\bibitem{statsmodelslibrary} \href{http://statsmodels.sourceforge.net/}{http://statsmodels.sourceforge.net/}
\bibitem{ref1} \href{http://statsmodels.sourceforge.net/notebooks/generated/statsmodels.stats.diagnostic.normal\_ad.html}{http://statsmodels.sourceforge.net/notebooks/generated/statsmodels.stats.diagnostic.normal\_ad.html}
\bibitem{ref} \href{http://matthieustigler.github.io/Lectures/Lect2ARMA.pdf}{http://matthieustigler.github.io/Lectures/Lect2ARMA.pdf}
\bibitem{Quandl'sStevenContinuousSeries} \href{https://www.quandl.com/data/SCF/documentation/about}{https://www.quandl.com/data/SCF/documentation/about}
\bibitem{ref} \href{https://en.wikipedia.org/wiki/Information\_criterion}{https://en.wikipedia.org/wiki/Information\_criterion}
\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%  APPENDIX

    
%    \section{Appendix}\label{appendix}
    
\begin{appendices}    
    
    \chapter{} \label{appendixA}
    
    \section{Multivariate Regression}\label{multivariate-regression}
    
Also known as `generalised linear model', it generalises linear
regression to multiple input variables (regressors) and \(n\)
observations. It is best expressed in matrix form as:
\begin{equation}
Y = X \beta + \epsilon
\end{equation}
where \(Y\) is a vector representing the endogenous (dependent)
variables, \(X\) is a matrix representing the exogenous (independent)
variables, \(\beta\) is the coefficients vector and \(\epsilon\) the
residuals vector.
The OLS method, which minimises the sum of squared residuals via the
Maximum Likelihood Estimation method (MLE), is used to estimate the
parameters:
\begin{equation}
\hat{\beta} = (X^\prime X)^{-1} X^\prime Y \\
\hat{\epsilon} = Y - X \hat{\beta}
\end{equation}
The covariance matrix of the residuals estimate is:
\begin{equation}
\hat{\Sigma} = scale \times  \sum \epsilon \epsilon^\prime
\end{equation} where \(scale = 1/ n\) using the MLE estimator or
\(scale= 1/ (n - kp)\) using the OLS estimator for a model with \(k\)
variables and \(p\) lags.
The covariance matrix for the coefficients is:
\begin{equation}
(X X^\prime)^{-1} \otimes \hat{\Sigma}
\end{equation} where \(\otimes\) is the \emph{Kronecker product}.
The Log Likelihood Function for OLS is:
\begin{equation}
\log(L) = -\frac{n}{2}\log (2 \pi) -\frac{n}{2}\log | \hat{\Sigma}| - \frac{n}{2}
\end{equation}
The variance of the residuals and parameters are therefore the diagonal
elements of the corresponding covariance matrices, from which the
standard errors can be calculated.
These are the conventions used in the script \emph{analysis.py}.
Additional mathematical details can be found in reference \cite{wikiVAR}.
The are several assumptions about the nature of the variables in the
model to hold. In particular, these should be \emph{stationary} and the
\(\epsilon\) homoscedastic (with finite variance) and normally
distributed.
The main applications of the multivariate regression are: * Vector
Autoregression Models - also known as VAR(p), these can be used to
forecast, test for stationarity (e.g.~ADF test) or model stationary
series like returns * Error Correction Model - also known as ECM, these
are used to model series which aren't stationary or that have stochastic
trends, like prices


    \subsection{Autoregression Models - AR(p)}\label{autoregression-models---arp}
    
Also referred to as VAR(p) where \emph{p} is the lag order, it is
simply a linear regression on a time series and its lagged (past)
values:
\begin{equation}
Y_t = c + \sum^{p}_{i=1} \phi_i Y_{t-i} + \epsilon_t \\
\end{equation}
where c is a constant (also known as the drift term), \(\phi_i\) are the
parameters of the model and \(\epsilon_t\) is the error term. Whether to
exclude the constant \(c\) or not depends on the nature of what we are
trying to model.
Computationally, the model can be fitted in one go by using the OLS
method described above with a special matrix formulation. Example code
for this can be found in \emph{analysis.py} in the project repository.
As shown in Section X, the VAR(p) system can be re-written in terms of
differences and lagged differences, which is how it is commonly expressed
in some cases, for example in the ADF test. See more details in references \cite{wikiVAR} and \cite{wikiAR}.


    \subsubsection{Dickey-Fuller Test and ADF}\label{dickey-fuller-test-and-adf}
    
The \emph{Dickey-Fuller test} examines the null hypothesis of whether a
unit root is present in the autoregressive model (also known as AR(p),
see Appendix) of the time series. For example, a simple AR(1) model is:
\begin{equation}
Y_t = \beta Y_{t-1} + \epsilon_t
\end{equation}
If \(\beta=1\) the series is said to have a `unit root' and hence is
non-stationary. The equation can be re-written as:
\begin{equation}
\Delta Y_t = (\beta - 1) Y_{t-1} + \epsilon_t = \phi Y_{t-1} + \epsilon_t
\end{equation}
where \(\phi= \beta-1\). Hence, testing for unit root is equivalent to
testing \(\phi=0\).
The value of the test $\text{statistic}(\hat{\phi})/\text{std.err}(\hat{\phi})$ is
then compared to the relevant critical values for the Dickey-Fuller
distribution. If found lower, then the null hypothesis \(\phi=0\) is
rejected and the series can be considered stationary.
There are three main versions of the test depending on whether drift and/or
time-dependent terms are included:
\begin{itemize}
\tightlist
\item
  Test for a unit root: \(\Delta Y_t = \phi Y_{t-1} + \epsilon_t\)
\item
  Test for a unit root with drift:
  \(\Delta Y_t = c_0 + \phi Y_{t-1} + \epsilon_t\)
\item
  Test for a unit root with drift and deterministic time-trend:
  \(\Delta Y_t = c_0 + c_1 t + \phi Y_{t-1} + \epsilon_t\)
\end{itemize}
Each version of the test has its own critical value which depends on
the size of the sample. Which version to use is not straightforward and
the wrong choice can lead to wrong conclusions. In general, financial
time series exclude the time-trend.
There is an extension to the test called the \textbf{Augmented
Dickey--Fuller test (ADF)}, which removes autocorrelation effects by
including lagged difference terms \(\phi_p \Delta Y_{t-p}\). The optimal
lag order could then be determined from the information criteria (see
above).
Clearly the above equations belong to the family of generalised linear
models, which means the parameters can be estimated using the familiar
linear regression described above.
Additional details on this topic can be found in references \cite{wikiADF}, \cite{wikiDF} and the Appendix.

    \subsubsection{Optimal Lag Order}\label{optimal-lag-order}
    
To select the optimal lag order, one approach uses the Akaike
Information Criterion (AIC). Iterating over different lag orders, the
one yielding the lowest value of AIC is selected. Statsmodels suggests to
try up to a maximum lag order of \(12*(n/100)^{1/4}\) where \(n\) is the
number of observations.
There are different definitions of AIC used - we use the same as in
statstools \cite{statstools},
which has different definitions for AR(p) and the ADF test as:
\begin{equation}
AIC = \log | \hat{\Sigma} |+ 2 \frac{1 + k}{n} \qquad \mathrm{(AR(p) \ model)}
\end{equation}
\begin{equation}
AIC = -2 \log(L) + 2k  \qquad \mathrm{(ADF \ test)} 
\end{equation}
where \(k\) is the number of estimated parameters. Other information
criteria can be used, see for example reference \cite{ref}.

    \subsubsection{Stability Condition}\label{stability-condition}
    
It is required for the eigenvalues of the estimated coefficients matrix or
vector to be inside the unit circle ($ \textless{} 1$): \begin{equation}
| \lambda I - \hat{\beta} | = 0
\end{equation} This is equivalent to requiring the roots of the characteristic
polynomial of the AR(p) system to be outside the unit circle - see
reference \cite{ref}.

\chapter{} \label{appendixB}

    \section{Cointegration between Italian and Dutch Gas}\label{cointegration-between-italian-and-dutch-gas}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_111_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_113_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Coint_Brent_Gasoil_v2_files/Coint_Brent_Gasoil_v2_114_0.png}
    \end{center}
    { \hspace*{\fill} \\}

  \end{appendices}

    % Add a bibliography block to the postdoc




    \end{document}
